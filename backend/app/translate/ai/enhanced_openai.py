"""
å¢å¼ºçš„ AI ç¿»è¯‘æœåŠ¡
æ”¯æŒç¼“å­˜ã€å¤‡ä»½æ¨¡å‹ã€é‡è¯•æœºåˆ¶
"""
import hashlib
import asyncio
import re
import time
from typing import List, Optional, Dict, Any
from datetime import datetime
from openai import AsyncOpenAI, OpenAI
from sqlalchemy.orm import Session

from ...models.translate_log import TranslateLog


class EnhancedAITranslator:
    """
    å¢å¼ºçš„ AI ç¿»è¯‘å™¨

    æ–°å¢åŠŸèƒ½ï¼š
    - ç¿»è¯‘ç»“æœç¼“å­˜ï¼ˆé¿å…é‡å¤ç¿»è¯‘ï¼‰
    - å¤‡ä»½æ¨¡å‹æ”¯æŒï¼ˆä¸»æ¨¡å‹å¤±è´¥æ—¶è‡ªåŠ¨åˆ‡æ¢ï¼‰
    - è‡ªåŠ¨é‡è¯•æœºåˆ¶ï¼ˆæœ€å¤š3æ¬¡ï¼‰
    - DeepSeek æ€è€ƒè¿‡ç¨‹è¿‡æ»¤
    - é€Ÿç‡é™åˆ¶å¤„ç†
    """

    def __init__(
        self,
        api_key: str,
        api_base: str = "https://api.openai.com/v1",
        model: str = "gpt-3.5-turbo",
        backup_model: Optional[str] = None,
        timeout: int = 60,
        db: Optional[Session] = None
    ):
        """
        åˆå§‹åŒ–å¢å¼ºçš„ AI ç¿»è¯‘å™¨

        Args:
            api_key: API å¯†é’¥
            api_base: API åŸºç¡€ URL
            model: ä¸»æ¨¡å‹
            backup_model: å¤‡ä»½æ¨¡å‹
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            db: æ•°æ®åº“ä¼šè¯ï¼ˆç”¨äºç¼“å­˜ï¼‰
        """
        self.api_key = api_key
        self.api_base = api_base
        self.model = model
        self.backup_model = backup_model
        self.timeout = timeout
        self.db = db

        # å½“å‰ä½¿ç”¨çš„æ¨¡å‹ï¼ˆå¯èƒ½æ˜¯å¤‡ä»½æ¨¡å‹ï¼‰
        self.current_model = model

        # åˆ›å»ºåŒæ­¥å’Œå¼‚æ­¥å®¢æˆ·ç«¯
        self.client = OpenAI(
            api_key=api_key,
            base_url=api_base,
            timeout=timeout
        )
        self.async_client = AsyncOpenAI(
            api_key=api_key,
            base_url=api_base,
            timeout=timeout
        )

    def _generate_md5_key(self, text: str, target_lang: str) -> str:
        """
        ç”Ÿæˆ MD5 å“ˆå¸Œé”®ç”¨äºç¼“å­˜

        Args:
            text: åŸæ–‡
            target_lang: ç›®æ ‡è¯­è¨€

        Returns:
            str: MD5 å“ˆå¸Œå€¼
        """
        content = f"{self.api_key}{self.api_base}{text}{self.model}{self.backup_model}{target_lang}"
        return hashlib.md5(content.encode('utf-8')).hexdigest()

    def _check_cache(self, text: str, target_lang: str) -> Optional[str]:
        """
        æ£€æŸ¥ç¼“å­˜ä¸­æ˜¯å¦æœ‰ç¿»è¯‘ç»“æœ

        Args:
            text: åŸæ–‡
            target_lang: ç›®æ ‡è¯­è¨€

        Returns:
            ç¼“å­˜çš„ç¿»è¯‘ç»“æœï¼Œå¦‚æœæ²¡æœ‰è¿”å› None
        """
        if not self.db:
            return None

        try:
            md5_key = self._generate_md5_key(text, target_lang)
            log = self.db.query(TranslateLog).filter_by(md5_key=md5_key).first()

            if log:
                print(f"âœ… å‘½ä¸­ç¼“å­˜: {text[:30]}...")
                return log.content

            return None
        except Exception as e:
            print(f"âŒ ç¼“å­˜æŸ¥è¯¢å¤±è´¥: {str(e)}")
            return None

    def _save_cache(self, text: str, target_lang: str, content: str):
        """
        ä¿å­˜ç¿»è¯‘ç»“æœåˆ°ç¼“å­˜

        Args:
            text: åŸæ–‡
            target_lang: ç›®æ ‡è¯­è¨€
            content: è¯‘æ–‡
        """
        if not self.db:
            return

        try:
            md5_key = self._generate_md5_key(text, target_lang)
            log = TranslateLog(
                md5_key=md5_key,
                api_url=self.api_base,
                api_key=self.api_key,
                model=self.model,
                backup_model=self.backup_model,
                target_lang=target_lang,
                source=text,
                content=content
            )
            self.db.add(log)
            self.db.commit()
        except Exception as e:
            print(f"âŒ ç¼“å­˜ä¿å­˜å¤±è´¥: {str(e)}")

    def _filter_deepseek_thought(self, text: str) -> str:
        """
        è¿‡æ»¤ DeepSeek æ€è€ƒè¿‡ç¨‹æ ‡ç­¾

        Args:
            text: ç¿»è¯‘ç»“æœ

        Returns:
            è¿‡æ»¤åçš„æ–‡æœ¬
        """
        # ç§»é™¤ <think>...</think> æ ‡ç­¾åŠå…¶å†…å®¹
        pattern = r'<think>.*?</think>'
        return re.sub(pattern, '', text, flags=re.DOTALL).strip()

    def _build_translation_prompt(self, text: str, target_lang: str) -> str:
        """
        æ„å»ºç¿»è¯‘æç¤ºè¯

        Args:
            text: è¦ç¿»è¯‘çš„æ–‡æœ¬
            target_lang: ç›®æ ‡è¯­è¨€

        Returns:
            str: æç¤ºè¯
        """
        # è¯­è¨€ä»£ç æ˜ å°„
        lang_names = {
            "zh": "ä¸­æ–‡",
            "en": "è‹±æ–‡",
            "ja": "æ—¥æ–‡",
            "ko": "éŸ©æ–‡",
            "fr": "æ³•æ–‡",
            "de": "å¾·æ–‡",
            "es": "è¥¿ç­ç‰™æ–‡",
            "ru": "ä¿„æ–‡"
        }

        target_name = lang_names.get(target_lang, target_lang)

        return f"è¯·å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆ{target_name}ï¼Œåªè¿”å›ç¿»è¯‘ç»“æœï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šï¼š\n\n{text}"

    def _call_openai_api(self, text: str, target_lang: str, use_backup: bool = False) -> str:
        """
        è°ƒç”¨ OpenAI API è¿›è¡Œç¿»è¯‘

        Args:
            text: è¦ç¿»è¯‘çš„æ–‡æœ¬
            target_lang: ç›®æ ‡è¯­è¨€
            use_backup: æ˜¯å¦ä½¿ç”¨å¤‡ä»½æ¨¡å‹

        Returns:
            str: ç¿»è¯‘ç»“æœ
        """
        model = self.backup_model if use_backup else self.current_model
        prompt = self._build_translation_prompt(text, target_lang)

        try:
            response = self.client.chat.completions.create(
                model=model,
                messages=[
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¿»è¯‘åŠ©æ‰‹ã€‚è¯·å‡†ç¡®ç¿»è¯‘ç”¨æˆ·æä¾›çš„æ–‡æœ¬ï¼Œä¿æŒåŸæ–‡çš„æ„æ€å’Œè¯­æ°”ã€‚"
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.3,
                max_tokens=4000
            )

            content = response.choices[0].message.content.strip()

            # è¿‡æ»¤ DeepSeek æ€è€ƒè¿‡ç¨‹
            content = self._filter_deepseek_thought(content)

            print(f"âœ… ç¿»è¯‘æˆåŠŸï¼ˆ{model}ï¼‰: {text[:30]}...")

            return content

        except Exception as e:
            print(f"âŒ API è°ƒç”¨å¤±è´¥ï¼ˆ{model}ï¼‰: {str(e)}")
            raise

    def translate_text(self, text: str, target_lang: str) -> str:
        """
        ç¿»è¯‘å•ä¸ªæ–‡æœ¬ï¼ˆå¸¦ç¼“å­˜ã€é‡è¯•ã€å¤‡ä»½æ¨¡å‹ï¼‰

        Args:
            text: è¦ç¿»è¯‘çš„æ–‡æœ¬
            target_lang: ç›®æ ‡è¯­è¨€

        Returns:
            str: ç¿»è¯‘ç»“æœ
        """
        if not text or not text.strip():
            return text

        # 1. æ£€æŸ¥ç¼“å­˜
        cached = self._check_cache(text, target_lang)
        if cached:
            return cached

        # 2. å°è¯•ç¿»è¯‘ï¼ˆå¸¦é‡è¯•å’Œå¤‡ä»½æ¨¡å‹ï¼‰
        max_retries = 3
        last_error = None

        for attempt in range(max_retries):
            try:
                # å…ˆå°è¯•ä¸»æ¨¡å‹
                content = self._call_openai_api(text, target_lang, use_backup=False)

                # ä¿å­˜åˆ°ç¼“å­˜
                self._save_cache(text, target_lang, content)

                return content

            except Exception as e:
                last_error = str(e)
                error_type = type(e).__name__

                print(f"âš ï¸  ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥ ({error_type}): {last_error}")

                # å¦‚æœæ˜¯é€Ÿç‡é™åˆ¶æˆ–è®¤è¯é”™è¯¯ï¼Œå°è¯•å¤‡ä»½æ¨¡å‹
                if error_type in ['RateLimitError', 'AuthenticationError', 'PermissionDeniedError']:
                    if self.backup_model and self.current_model != self.backup_model:
                        print(f"ğŸ”„ åˆ‡æ¢åˆ°å¤‡ä»½æ¨¡å‹: {self.backup_model}")
                        self.current_model = self.backup_model

                        # ç­‰å¾…1ç§’åé‡è¯•
                        time.sleep(1)

                        # ä½¿ç”¨å¤‡ä»½æ¨¡å‹é‡è¯•
                        content = self._call_openai_api(text, target_lang, use_backup=True)

                        # ä¿å­˜åˆ°ç¼“å­˜
                        self._save_cache(text, target_lang, content)

                        # æ¢å¤ä¸»æ¨¡å‹
                        self.current_model = self.model

                        return content

                # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥
                if attempt == max_retries - 1:
                    print(f"âŒ ç¿»è¯‘æœ€ç»ˆå¤±è´¥ï¼Œè¿”å›åŸæ–‡: {text[:30]}...")
                    return text

                # ç­‰å¾…5ç§’åé‡è¯•
                time.sleep(5)

        return text

    def translate_batch(self, texts: List[str], target_lang: str) -> List[str]:
        """
        æ‰¹é‡ç¿»è¯‘æ–‡æœ¬

        Args:
            texts: è¦ç¿»è¯‘çš„æ–‡æœ¬åˆ—è¡¨
            target_lang: ç›®æ ‡è¯­è¨€

        Returns:
            List[str]: ç¿»è¯‘ç»“æœåˆ—è¡¨
        """
        results = []

        for text in texts:
            if text and text.strip():
                translated = self.translate_text(text, target_lang)
                results.append(translated)
            else:
                results.append(text)

        return results

    async def _call_openai_api_async(self, text: str, target_lang: str, use_backup: bool = False) -> str:
        """
        å¼‚æ­¥è°ƒç”¨ OpenAI API è¿›è¡Œç¿»è¯‘

        Args:
            text: è¦ç¿»è¯‘çš„æ–‡æœ¬
            target_lang: ç›®æ ‡è¯­è¨€
            use_backup: æ˜¯å¦ä½¿ç”¨å¤‡ä»½æ¨¡å‹

        Returns:
            str: ç¿»è¯‘ç»“æœ
        """
        model = self.backup_model if use_backup else self.current_model
        prompt = self._build_translation_prompt(text, target_lang)

        try:
            response = await self.async_client.chat.completions.create(
                model=model,
                messages=[
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¿»è¯‘åŠ©æ‰‹ã€‚è¯·å‡†ç¡®ç¿»è¯‘ç”¨æˆ·æä¾›çš„æ–‡æœ¬ï¼Œä¿æŒåŸæ–‡çš„æ„æ€å’Œè¯­æ°”ã€‚"
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.3,
                max_tokens=4000
            )

            content = response.choices[0].message.content.strip()

            # è¿‡æ»¤ DeepSeek æ€è€ƒè¿‡ç¨‹
            content = self._filter_deepseek_thought(content)

            print(f"âœ… ç¿»è¯‘æˆåŠŸï¼ˆ{model}ï¼‰: {text[:30]}...")

            return content

        except Exception as e:
            print(f"âŒ API è°ƒç”¨å¤±è´¥ï¼ˆ{model}ï¼‰: {str(e)}")
            raise

    async def translate_text_async(self, text: str, target_lang: str) -> str:
        """
        å¼‚æ­¥ç¿»è¯‘å•ä¸ªæ–‡æœ¬ï¼ˆå¸¦ç¼“å­˜ã€é‡è¯•ã€å¤‡ä»½æ¨¡å‹ï¼‰

        Args:
            text: è¦ç¿»è¯‘çš„æ–‡æœ¬
            target_lang: ç›®æ ‡è¯­è¨€

        Returns:
            str: ç¿»è¯‘ç»“æœ
        """
        if not text or not text.strip():
            return text

        # 1. æ£€æŸ¥ç¼“å­˜
        cached = self._check_cache(text, target_lang)
        if cached:
            return cached

        # 2. å°è¯•ç¿»è¯‘ï¼ˆå¸¦é‡è¯•å’Œå¤‡ä»½æ¨¡å‹ï¼‰
        max_retries = 3
        last_error = None

        for attempt in range(max_retries):
            try:
                # å…ˆå°è¯•ä¸»æ¨¡å‹
                content = await self._call_openai_api_async(text, target_lang, use_backup=False)

                # ä¿å­˜åˆ°ç¼“å­˜
                self._save_cache(text, target_lang, content)

                return content

            except Exception as e:
                last_error = str(e)
                error_type = type(e).__name__

                print(f"âš ï¸  ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥ ({error_type}): {last_error}")

                # å¦‚æœæ˜¯é€Ÿç‡é™åˆ¶æˆ–è®¤è¯é”™è¯¯ï¼Œå°è¯•å¤‡ä»½æ¨¡å‹
                if error_type in ['RateLimitError', 'AuthenticationError', 'PermissionDeniedError']:
                    if self.backup_model and self.current_model != self.backup_model:
                        print(f"ğŸ”„ åˆ‡æ¢åˆ°å¤‡ä»½æ¨¡å‹: {self.backup_model}")
                        self.current_model = self.backup_model

                        # ç­‰å¾…1ç§’åé‡è¯•
                        await asyncio.sleep(1)

                        # ä½¿ç”¨å¤‡ä»½æ¨¡å‹é‡è¯•
                        content = await self._call_openai_api_async(text, target_lang, use_backup=True)

                        # ä¿å­˜åˆ°ç¼“å­˜
                        self._save_cache(text, target_lang, content)

                        # æ¢å¤ä¸»æ¨¡å‹
                        self.current_model = self.model

                        return content

                # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥
                if attempt == max_retries - 1:
                    print(f"âŒ ç¿»è¯‘æœ€ç»ˆå¤±è´¥ï¼Œè¿”å›åŸæ–‡: {text[:30]}...")
                    return text

                # ç­‰å¾…5ç§’åé‡è¯•
                await asyncio.sleep(5)

        return text

    async def translate_batch_async_concurrent(
        self,
        texts: List[str],
        target_lang: str,
        max_concurrency: int = 5,
        progress_callback: Optional[callable] = None
    ) -> List[str]:
        """
        å¹¶å‘æ‰¹é‡ç¿»è¯‘ï¼Œä½¿ç”¨ Semaphore æ§åˆ¶å¹¶å‘æ•°

        Args:
            texts: è¦ç¿»è¯‘çš„æ–‡æœ¬åˆ—è¡¨
            target_lang: ç›®æ ‡è¯­è¨€
            max_concurrency: æœ€å¤§å¹¶å‘æ•°
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°

        Returns:
            List[str]: ç¿»è¯‘ç»“æœåˆ—è¡¨ï¼ˆä¿æŒåŸå§‹é¡ºåºï¼‰
        """
        # åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°
        semaphore = asyncio.Semaphore(max_concurrency)

        async def translate_with_semaphore(text: str, index: int) -> tuple[int, str]:
            """åœ¨ä¿¡å·é‡æ§åˆ¶ä¸‹è¿›è¡Œç¿»è¯‘"""
            async with semaphore:
                result = await self.translate_text_async(text, target_lang)
                if progress_callback:
                    progress_callback(index + 1, len(texts))
                return (index, result)

        # åˆ›å»ºæ‰€æœ‰ç¿»è¯‘ä»»åŠ¡
        tasks = [
            translate_with_semaphore(text, i)
            for i, text in enumerate(texts)
            if text and text.strip()
        ]

        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        results = await asyncio.gather(*tasks)

        # æŒ‰åŸå§‹é¡ºåºè¿”å›ç»“æœ
        sorted_results = list(texts)
        for index, result in results:
            sorted_results[index] = result

        return sorted_results
